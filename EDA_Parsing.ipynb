{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1:\n",
    "\n",
    "### Extract 10-K text and semgment into sections designated by table of contents for a specific filing document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_tenk_text_dict = get_text_dict(r\"https://www.sec.gov/Archives/edgar/data/1318605/000119312511054847/0001193125-11-054847.txt\", '10-K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parsed_tenk_text_dict.keys()\n",
    "# parsed_tenk_text_dict['Controls and Procedures']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_tenk_text_dict['Market for Registrant\\x92s Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_eightk = get_text_dict(r'https://www.sec.gov/Archives/edgar/data/1318605/000156459021012981/0001564590-21-012981.txt', '8-K')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have text organized into these sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in parsed_tenk_text_dict.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We also want to do the same with 10-Q forms for quarterly reports. \n",
    "### Part of the analysis will be comparing 10-Qs with past 10-Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "\n",
    "Will compare the cosine similarity from previous doc sections to current sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing nlp pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
    "    return only_ascii.decode()\n",
    "\n",
    "input_string = remove_accents(parsed_tenk_text_dict['CONTROLS AND PROCEDURES '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokens = sent_tokenize(input_string)\n",
    "\n",
    "sent_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk.tokenize.word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = [word_tokenize(sent) for sent in sent_tokens]\n",
    "\n",
    "list(enumerate(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make lowercase\n",
    "\n",
    "import string\n",
    "\n",
    "tokens_lower = [[word.lower() for word in sent]\n",
    "                 for sent in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter stopwords and punctuation\n",
    "\n",
    "### will need to go back and create custom stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_ = set(stopwords.words('english'))\n",
    "punctuation_ = set(string.punctuation)\n",
    "print(\"--- punctuation: {}\".format(string.punctuation))\n",
    "\n",
    "def filter_tokens(sent):\n",
    "    return([w for w in sent if not w in stopwords_ and not w in punctuation_])\n",
    "\n",
    "tokens_filtered = list(map(filter_tokens, tokens_lower))\n",
    "\n",
    "for sent in tokens_filtered:\n",
    "    print(\"--- sentence tokens: {}\".format(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "sentence_number = 0\n",
    "\n",
    "stemmer_porter = PorterStemmer()\n",
    "tokens_stemporter = [list(map(stemmer_porter.stem, sent)) for sent in tokens_filtered]\n",
    "print(\"--- sentence tokens (porter): {}\".format(tokens_stemporter[sentence_number]))\n",
    "\n",
    "stemmer_snowball = SnowballStemmer('english')\n",
    "tokens_stemsnowball = [list(map(stemmer_snowball.stem, sent)) for sent in tokens_filtered]\n",
    "print(\"--- sentence tokens (snowball): {}\".format(tokens_stemsnowball[sentence_number]))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens_lemmatize = [list(map(lemmatizer.lemmatize, sent)) for sent in tokens_filtered]\n",
    "print(\"--- sentence tokens (lemmatize): {}\".format(tokens_lemmatize[sentence_number]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
